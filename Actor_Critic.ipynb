{"cells":[{"cell_type":"markdown","metadata":{"id":"rqoBGzwhpBrA"},"source":["# Prepare"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25019,"status":"ok","timestamp":1678910980917,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"QoM7l_KXo39f","outputId":"729ea360-13df-4983-9bf6-64c195d7b495"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141656,"status":"ok","timestamp":1678911125626,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"i8a7qwt4o-SC","outputId":"ec31dad5-e7ac-45d2-f8e1-4c946b6aff90"},"outputs":[],"source":["!pip install gym[atari]\n","!pip install autorom[accept-rom-license]"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6703,"status":"ok","timestamp":1678911150387,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"7lCzbufgpCvz"},"outputs":[],"source":["import gym\n","import numpy as np\n","from collections import deque\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import time\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","from IPython import display"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2974,"status":"ok","timestamp":1678911156547,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"KJcHKQgrq10M"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","\n","from collections import namedtuple\n","\n","import copy\n","from tqdm.notebook import tqdm\n","import random"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8gJSIqWSpJys"},"source":["# Wrappers"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1038,"status":"ok","timestamp":1678911161264,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"R9fFNOi8pGpT"},"outputs":[],"source":["class ConcatObs(gym.Wrapper):\n","    def __init__(self, env, k=4):\n","        gym.Wrapper.__init__(self, env)\n","        self.k = k\n","        self.frames = deque([], maxlen=k)\n","        shp = env.observation_space.shape\n","        self.observation_space = gym.spaces.Box(low=0, high=255, shape=((k,) + shp), dtype=env.observation_space.dtype)\n","\n","    def reset(self):\n","        ob = self.env.reset()\n","        for _ in range(self.k):\n","            self.frames.append(ob)\n","\n","        return self._get_ob()\n","\n","    def step(self, action):\n","        total_reward = 0.0\n","        done = None\n","        for i in range(self.k):\n","            obs, reward, done, info = self.env.step(action)\n","            total_reward += reward\n","            self.frames.append(obs)\n","            \n","            # Only count one live each episode\n","            done = True if info['lives'] < 4 else False\n","            if done:  \n","                break\n","        return self._get_ob(), total_reward, done, info\n","\n","    def _get_ob(self):\n","        return np.array(self.frames)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1678911161264,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"Uza878typJQs"},"outputs":[],"source":["# A bunch of wrappers to get us started, please use these\n","class ObservationWrapper(gym.ObservationWrapper):\n","    def __init__(self, env, GRAYSCALE=False, NORMALIZE=False):\n","        self.GRAYSCALE = GRAYSCALE\n","        self.NORMALIZE = NORMALIZE\n","        super().__init__(env)\n","    \n","    def observation(self, obs):\n","        # Normalise observation by 255\n","        if self.NORMALIZE:\n","            obs = obs / 255.0\n","            \n","        if self.GRAYSCALE:\n","            obs = tf.image.rgb_to_grayscale(obs)\n","                    \n","        image = obs[:,2:-9,8:,:]\n","\n","        image = tf.image.resize(image,[84,84])\n","        # print(image.shape)\n","        # (4, 84, 84, 1)\n","\n","        image = tf.reshape(image, image.shape[:-1])\n","        # print(image.shape)\n","        # (4, 84, 84)\n","        \n","        # image = tf.transpose(image,perm = [1,2,0])\n","        # print(image.shape)\n","        # (84, 84, 4)\n","\n","        return image\n","\n","class RewardWrapper(gym.RewardWrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","    \n","    def reward(self, reward):\n","        # Clip reward between 0 to 1\n","        #return np.clip(reward, 0, 1)\n","        return reward\n","    \n","class ActionWrapper(gym.ActionWrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","    \n","    def action(self, action):\n","        return action\n","\n","class FireResetEnv(gym.Wrapper):\n","    def __init__(self, env):\n","        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n","        super().__init__(env)\n","        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n","        assert len(env.unwrapped.get_action_meanings()) >= 3\n","\n","    def reset(self, **kwargs):\n","        self.env.reset(**kwargs)\n","        obs, _, done, _ = self.env.step(1)\n","        if done:\n","            self.env.reset(**kwargs)\n","        obs, _, done, _ = self.env.step(2)\n","        if done:\n","            self.env.reset(**kwargs)\n","        return obs\n","\n","    def step(self, ac):\n","        return self.env.step(ac)"]},{"cell_type":"markdown","metadata":{"id":"0gvh3srMpUfA"},"source":["# Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":470,"status":"ok","timestamp":1678911164024,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"ZnEFnW7tpWRZ","outputId":"8364e860-e811-4fb4-8e9e-2e9d01037f31"},"outputs":[],"source":["# env = gym.make(\"ALE/Breakout-v5\")\n","env = gym.make(\"ALE/Riverraid-v5\")\n","\n","# Use wrappers for the environment\n","env = ObservationWrapper(RewardWrapper(ActionWrapper(ConcatObs(FireResetEnv(env),k=4))), GRAYSCALE=True, NORMALIZE=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5639,"status":"ok","timestamp":1678911169659,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"1d-wRevEEVkz","outputId":"ab38d404-acf9-4fa1-ed1a-849792279874"},"outputs":[],"source":["obs = env.reset()\n","print(obs.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8UkgKwV0pZlO"},"source":["# Actor Critic"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1678911172137,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"J-Uw9gWYpZGN"},"outputs":[],"source":["# Hyper parameters\n","INPUT_SHAPE = obs.shape\n","N_ACTION = env.action_space.n\n","IMG_SIZE = INPUT_SHAPE[0]\n","IMG_CHANNELS = INPUT_SHAPE[2]\n","\n","EPISODES = 100    # how many episode will run\n","MAX_STEP = 100000 # how many step in one episode\n","LR = 1e-3         # learning rate\n","HIDDEN = 512      # hidden layer \n","MEM_SIZE = 10000  # memory size for transition\n","BATCH_SIZE = 64   # batch size for one gradient update\n","GAMMA = 0.99      # discount reward factor\n","RUNNING_REWARD = 10\n","\n","ADVANTAGE = True  # if use A2C"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678911175385,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"v2i0Pii-J7nU","outputId":"d6420192-2c0f-496b-89c5-32e5022583ff"},"outputs":[],"source":["DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":502,"status":"ok","timestamp":1678911179878,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"qgGMGmqq4Fyz","outputId":"06e2ed1f-ace0-4541-a10a-51b779c4be5a"},"outputs":[],"source":["print(INPUT_SHAPE)\n","print(IMG_SIZE)\n","print(IMG_CHANNELS)\n","print(N_ACTION)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1678911181829,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"g7ca8MwtGV4N"},"outputs":[],"source":["def process_state(state):\n","    state_ = np.array(state)\n","    # state_ = state_.reshape((IMG_CHANNELS, IMG_SIZE, IMG_SIZE))\n","    state_ = torch.from_numpy(state_).float().to(DEVICE)\n","\n","    # Output size: torch.Size([4, 84, 84])\n","\n","    return state_"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":512,"status":"ok","timestamp":1678911186437,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"aBA_F6HHrS8x"},"outputs":[],"source":["SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n","\n","class ActorCritic(torch.nn.Module):\n","    def __init__(self, input_shape=INPUT_SHAPE, hidden_size=HIDDEN, n_action=N_ACTION, advantage=ADVANTAGE):\n","        super().__init__()\n","        self.input_shape = input_shape\n","        self.action_space = n_action\n","        self.advantage = advantage\n","\n","        # CNN\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n","            nn.ReLU(),\n","        )\n","\n","        # CNN output size\n","        CNN_output_size = 7 * 7 * 64\n","\n","        # Actor\n","        self.actor_fc1 = nn.Linear(CNN_output_size, hidden_size)\n","        self.actor_fc2 = nn.Linear(hidden_size, self.action_space)\n","\n","        # Critic\n","        self.critic_fc1 = nn.Linear(CNN_output_size, hidden_size)\n","        self.critic_fc2 = nn.Linear(hidden_size, 1)\n","\n","        # Optimizer\n","        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n","\n","        # Action & Reward buffer\n","        self.saved_actions = []\n","        self.rewards = []\n","\n","\n","    def forward(self, state):\n","        # CNN\n","        h = self.conv(state)\n","        # print(\"h\")\n","        # print(h)\n","        # print(h.shape)\n","\n","        # Flatten\n","        h = h.reshape((-1, 7*7*64))\n","        # print(\"h\")\n","        # print(h)\n","        # print(h.shape)\n","\n","        # Actor generate the probability for each action on given state\n","        probs = F.relu(self.actor_fc1(h))\n","        probs = self.actor_fc2(probs)\n","        # print(\"probs\")\n","        # print(probs)\n","        # print(probs.shape)\n","        probs = F.softmax(probs, dim=1)   # CAREFUL ABOUT THE DIM!!\n","        # print(probs)\n","        # print(probs.shape)\n","\n","        # Critic the V value for given state\n","        value = F.relu(self.critic_fc1(h))\n","        value = self.critic_fc2(value)\n","\n","        return value, probs\n","\n","\n","    def choose_action(self, state):\n","        value, probs = self(state)\n","        value = torch.squeeze(value)\n","\n","        # Create a categorical distribution over the list of probabilities of actions\n","        probs_dist = Categorical(probs)\n","\n","        # And sample an action using the distribution\n","        action = probs_dist.sample()\n","\n","        # Save to action buffer\n","        self.saved_actions.append(SavedAction(probs_dist.log_prob(action), value))\n","\n","        # The action to take\n","        return action.item()\n","\n","    \n","    def finish_episode(self):\n","        \"\"\"\n","        Training code. Calculates actor and critic loss and performs backprop.\n","        \"\"\"\n","        eps = np.finfo(np.float32).eps.item()\n","\n","        R = 0\n","        saved_actions = self.saved_actions\n","        policy_losses = [] # list to save actor (policy) loss\n","        value_losses = [] # list to save critic (value) loss\n","        returns = [] # list to save the true values\n","\n","        # Calculate the true value using rewards returned from the environment\n","        for r in self.rewards[::-1]:\n","            # Calculate the discounted value\n","            R = r + GAMMA * R\n","            returns.insert(0, R)\n","\n","        returns = torch.tensor(returns).to(DEVICE)\n","        returns = (returns - returns.mean()) / (returns.std() + eps)\n","\n","        for (log_prob, value), R in zip(saved_actions, returns):\n","            # Calculate actor (policy) loss\n","            if self.advantage:\n","                advantage = R - value.item()\n","                policy_losses.append(-log_prob * advantage)\n","            else:\n","                policy_losses.append(-log_prob * value.item())\n","\n","            # Calculate critic (value) loss using L1 smooth loss\n","            value_losses.append(F.smooth_l1_loss(value, R))\n","\n","        # Reset gradients\n","        self.optimizer.zero_grad()\n","\n","        # Sum up all the values of policy_losses and value_losses\n","        loss = torch.stack(policy_losses).mean() + torch.stack(value_losses).mean()\n","\n","        # Perform backprop\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # Reset rewards and action buffer\n","        del self.rewards[:]\n","        del self.saved_actions[:]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0RK6OVKxtiSm"},"source":["# Train with REINFORCE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":403,"status":"ok","timestamp":1678475471261,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"tT4mw2EEwVc-","outputId":"0f2ad675-671b-4bc6-b7b6-37c26deb6dc8"},"outputs":[],"source":["agent = ActorCritic().to(DEVICE)\n","print(agent.advantage)\n","\n","# Print model state\n","print(\"Model's state_dict:\")\n","for param_tensor in agent.state_dict():\n","    print(param_tensor, \"\\t\", agent.state_dict()[param_tensor].size())\n","\n","print()\n","\n","# Print optimizer state\n","print(\"Optimizer's state_dict:\")\n","for var_name in agent.optimizer.state_dict():\n","    print(var_name, \"\\t\", agent.optimizer.state_dict()[var_name])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["41e09cd5bb1e469aa46ac43f06bf9c85","4938f84e35544600879344f2f6c5e127","00f7ac4bedc945b8bca9f688b5b602b1","66bd9a1beacc419783ea62e041898512","3074789bf027471383d0e46af91708b5","880dc584314b4de9bcd128474a271586","374f96f8122d48689b0bc08cc1b2b5dd","2392f8b3f2cf41bbbce267baac80f293","510239ab1ceb4f39bf2767d2a23cc1bb","79bdf797765f4370b7544ef70b0c462e","5a991549c659463b8beb4a6109eb56ae"]},"executionInfo":{"elapsed":552234,"status":"ok","timestamp":1678476026962,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"l55s4561th2K","outputId":"464d6f9e-c889-4b27-8d9b-7eb159d08acf"},"outputs":[],"source":["reward_list = []\n","run_steps = []\n","\n","best_reward = 0\n","average_reward = 0\n","episode_number = []\n","average_reward_number = []\n","\n","start_time = time.time()\n","\n","# Train for EPISODES number\n","for i in tqdm(range(1, EPISODES)):\n","    # reset\n","    state = env.reset()\n","    state = process_state(state)\n","\n","    ep_reward = 0\n","\n","    # Step counter for one episode\n","    step = 0\n","\n","    # Start episode\n","    for step in range(MAX_STEP):\n","\n","        # print(\"state\")\n","        # print(state)\n","        # print(state.shape)\n","\n","        # Sample action\n","        action = agent.choose_action(state)\n","        # Step forward\n","        new_state, reward, done, info = env.step(action)\n","\n","        # Transform the state\n","        new_state = process_state(new_state)\n","\n","        #\n","        agent.rewards.append(reward)\n","        ep_reward += reward\n","\n","        # Update loop variable\n","        state = new_state\n","\n","        if step%100==0 and step!=0:\n","            print(\"   Current step {}/{}\".format(step, MAX_STEP))\n","\n","        if done or step == MAX_STEP-1:\n","            reward_list.append(ep_reward)\n","            run_steps.append(step)\n","\n","            if ep_reward > best_reward:\n","                best_reward = ep_reward\n","\n","            average_reward += ep_reward \n","\n","            if i%10==0:\n","                print(\"Episode {} Average Reward {} Best Reward {} Last Reward {}\".format(i, average_reward/i, best_reward, ep_reward))\n","\n","            break\n","\n","    # Update cumulative reward after one episode\n","    RUNNING_REWARD = 0.05 * ep_reward + (1 - 0.05) * RUNNING_REWARD\n","\n","    # Perform backprop\n","    agent.finish_episode()\n","  \n","    # Add the result of one episode\n","    episode_number.append(i)\n","    average_reward_number.append(average_reward/i)\n","\n","plt.plot(episode_number, average_reward_number)\n","plt.show()\n","\n","end_time = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":624},"executionInfo":{"elapsed":811,"status":"ok","timestamp":1678476042945,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"laol9MKI2QUW","outputId":"43fae29a-67eb-4c18-9dbf-71a1129c838b"},"outputs":[],"source":["runtime = end_time - start_time\n","print(\"Running time:\", runtime)\n","\n","average_reward = sum(reward_list)/EPISODES\n","max_reward = max(reward_list)\n","print('Average reward:', average_reward)\n","print('Max reward:', max_reward)\n","\n","plt.plot(reward_list)\n","plt.title('Reward for episodes')\n","plt.ylabel('Reward')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/rewardDDQN.jpg')\n","plt.show()\n","\n","plt.plot(run_steps, color='green')\n","plt.title('Run steps for episodes')\n","plt.ylabel('Steps')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/runtimeDDQN.jpg')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":361,"status":"ok","timestamp":1678476060595,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"KKXfWqOT2SFj","outputId":"00090c13-faeb-48ef-c38e-7f6cc612aca4"},"outputs":[],"source":["path = '/content/drive/MyDrive/INF581-RL-Shared/A'\n","if ADVANTAGE:\n","    path += '2'\n","path += 'C_parameter'+ str(average_reward) + '.pkl'\n","print(path)\n","\n","torch.save(agent.state_dict(), path)"]},{"cell_type":"markdown","metadata":{"id":"7MFHi-V_po9p"},"source":["# Train with Experience Replay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D5rxKSo1rBsy"},"outputs":[],"source":["class ReplayBuffer:\n","    def __init__(self):\n","        self.memory = deque(maxlen=MEM_SIZE)\n","    \n","    def add(self, experience):\n","        self.memory.append(experience)\n","    \n","    def sample(self):\n","        minibatch = random.sample(self.memory, BATCH_SIZE)\n","\n","        state1_batch = torch.stack([s1 for (s1,a,r,s2,d) in minibatch])\n","        action_batch = torch.tensor([a for (s1,a,r,s2,d) in minibatch])\n","        reward_batch = torch.tensor([r for (s1,a,r,s2,d) in minibatch])\n","        state2_batch = torch.stack([s2 for (s1,a,r,s2,d) in minibatch])\n","        done_batch = torch.tensor([d for (s1,a,r,s2,d) in minibatch])\n","\n","        return (state1_batch, action_batch, reward_batch, state2_batch, done_batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-qroBFrrX6m"},"outputs":[],"source":["class AnC:\n","    def __init__(self, advantage=ADVANTAGE):\n","        self.replay = ReplayBuffer()\n","        self.actor_critic = ActorCritic().to(DEVICE)\n","        self.advantage = advantage\n","\n","\n","    def choose_action(self, state):\n","        with torch.no_grad():\n","            # Probability for each action\n","            _, probs = self.actor_critic(state)\n","            # print(probs.requires_grad)\n","\n","        # Create a categorical distribution over the list of probabilities of actions\n","        p_a_s = Categorical(probs)  # p(a|s)\n","\n","        # And sample an action using the distribution\n","        action = p_a_s.sample()\n","\n","        # the action to take\n","        return action.item()\n","\n","\n","    def learn(self):\n","        if len(self.replay.memory)< BATCH_SIZE:\n","            return\n","\n","        # Sample minibatch s1, a1, r1, s1', done_1, ... , sn, an, rn, sn', done_n\n","        # Sample randomly\n","        state1_batch, action_batch, reward_batch, state2_batch, done_batch = self.replay.sample()\n","\n","        # Init\n","        policy_losses = []\n","        value_losses = []\n","\n","        for index in range(BATCH_SIZE):\n","            state = state1_batch[index]\n","            action = action_batch[index]\n","            reward = reward_batch[index]\n","            new_state = state2_batch[index]\n","            done = done_batch[index]\n","\n","            # If done, no future value should be added\n","            one_minus_done = 1 - int(done)\n","\n","            # Compute Q value and probs\n","            value, probs = self.actor_critic(state)\n","            value = torch.squeeze(value)\n","            probs_dist = Categorical(probs)\n","\n","            # The log probability for the action\n","            log_prob = probs_dist.log_prob(action.to(DEVICE))\n","\n","            # Compute Q value for new state\n","            with torch.no_grad():\n","                next_value, _ = self.actor_critic(new_state)\n","                next_value = torch.squeeze(next_value)\n","\n","            # TD learning\n","            target = reward + GAMMA * next_value * one_minus_done  # if done, no future value\n","\n","            # Calculate actor (policy) loss\n","            if self.advantage:\n","                advantage = target - value.item()\n","                policy_losses.append(-log_prob * advantage)\n","            else:\n","                policy_losses.append(-log_prob * value.item())\n","\n","            # Calculate critic (value) loss\n","            value_losses.append(F.smooth_l1_loss(value, target))\n","\n","\n","        # Reset gradients\n","        self.actor_critic.optimizer.zero_grad()\n","\n","        # Sum up all the values of policy_losses and value_losses\n","        loss = torch.stack(policy_losses).mean() + torch.stack(value_losses).mean()\n","\n","        # Perform backprop\n","        loss.backward()\n","        self.actor_critic.optimizer.step()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":428,"status":"ok","timestamp":1678540593435,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"geW0lEuVr0GE","outputId":"f038d560-59bf-40b0-9874-3a9adab320ec"},"outputs":[],"source":["agent = AnC()\n","\n","# Print model state\n","print(\"Model's state_dict:\")\n","for param_tensor in agent.actor_critic.state_dict():\n","    print(param_tensor, \"\\t\", agent.actor_critic.state_dict()[param_tensor].size())\n","\n","print()\n","\n","# Print optimizer state\n","print(\"Optimizer's state_dict:\")\n","for var_name in agent.actor_critic.optimizer.state_dict():\n","    print(var_name, \"\\t\", agent.actor_critic.optimizer.state_dict()[var_name])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450,"referenced_widgets":["ea735b5c7e024aa985123e8cf0b8a42f","07bda516d15041ae92b7a630e425213c","e938c4ea1b23483cbfd40a131485c6bd","3ba4943fdef4449e819c798d896ff86f","b63b9648473740b6b8a0b33c309790d9","d8ba2315a48b4957a715f6abaeadb9d3","dfcfe1b71b304c368ee4d490945a5a9c","3a81f32990c941db8c3aa92ad3e4f06c","eda2fae0dfc84f268f46e0fde7758e6f","28d749822e684c93a3e25b9cd9369c7e","5331f7b3adde4421aa1f0f490d2f9c27"]},"executionInfo":{"elapsed":197335,"status":"ok","timestamp":1678541099410,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"goOEDt3vrogo","outputId":"1591775e-212a-42c7-be91-578f0b8d4a76"},"outputs":[],"source":["reward_list = []\n","run_steps = []\n","\n","best_reward = 0\n","average_reward = 0\n","episode_number = []\n","average_reward_number = []\n","\n","start_time = time.time()\n","\n","# Train for EPISODES number\n","for i in tqdm(range(1, EPISODES)):\n","    # reset\n","    state = env.reset()\n","    state = process_state(state)\n","\n","    score = 0\n","    agent.nan_counter = 0\n","\n","    # Step counter for one episode\n","    step = 0\n","\n","    # Start episode\n","    for step in range(MAX_STEP):\n","\n","        # print(\"state\")\n","        # print(state)\n","        # print(state.shape)\n","\n","        # Sample action\n","        action = agent.choose_action(state)\n","        # Step forward\n","        new_state, reward, done, info = env.step(action)\n","\n","        # Transform the state\n","        new_state = process_state(new_state)\n","\n","        # Record this exp\n","        exp = (state, action, reward, new_state, done)\n","        agent.replay.add(exp)\n","        \n","        # Update NN\n","        agent.learn()\n","\n","        # Update loop variable\n","        state = new_state\n","        score += reward\n","\n","        if step%100==0 and step!=0:\n","            print(\"   Current step {}/{}\".format(step, MAX_STEP))\n","\n","        if done or step == MAX_STEP-1:\n","            reward_list.append(score)\n","            run_steps.append(step)\n","\n","            if score > best_reward:\n","                best_reward = score\n","\n","            average_reward += score \n","\n","            if i%10==0:\n","                print(\"Episode {} Average Reward {} Best Reward {} Last Reward {}\".format(i, average_reward/i, best_reward, score))\n","\n","            break\n","  \n","    episode_number.append(i)\n","    average_reward_number.append(average_reward/i)\n","\n","plt.plot(episode_number, average_reward_number)\n","plt.show()\n","\n","end_time = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":624},"executionInfo":{"elapsed":811,"status":"ok","timestamp":1678541110640,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"KFDcr-qqpwOl","outputId":"68401c7f-e91b-4208-f2c9-69be71614931"},"outputs":[],"source":["runtime = end_time - start_time\n","print(\"Running time:\", runtime)\n","\n","average_reward = sum(reward_list)/EPISODES\n","max_reward = max(reward_list)\n","print('Average reward:', average_reward)\n","print('Max reward', max_reward)\n","\n","plt.plot(reward_list)\n","plt.title('Reward for episodes')\n","plt.ylabel('Reward')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/rewardDDQN.jpg')\n","plt.show()\n","\n","plt.plot(run_steps, color='green')\n","plt.title('Run steps for episodes')\n","plt.ylabel('Steps')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/runtimeDDQN.jpg')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":428,"status":"ok","timestamp":1678540335797,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"DVXmQibddyBH","outputId":"be406673-05bd-41e8-d8f7-7a78bd21e09f"},"outputs":[],"source":["path = '/content/drive/MyDrive/INF581-RL-Shared/A'\n","if ADVANTAGE:\n","    path += '2'\n","path += 'C_parameter'+ str(average_reward) + '.pkl'\n","print(path)\n","\n","torch.save(agent.actor_critic.state_dict(), path)"]},{"cell_type":"markdown","metadata":{"id":"z81myPtDp8-P"},"source":["# Test"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1678913360418,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"JRRF6gXAp8lf"},"outputs":[],"source":["def show_frame(env, step=0, info=\"\"):\n","    plt.figure(3)\n","    plt.clf()\n","    plt.imshow(env.render(mode='rgb_array'))\n","    plt.title(\"%s | Step: %d %s\" % (env.spec.id, step, info))\n","    plt.axis('off')\n","\n","    display.clear_output(wait=True)\n","    display.display(plt.gcf())\n","\n","def save_gif(frames, path):\n","    patch = plt.imshow(frames[0])\n","    plt.axis(\"off\")\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval = 30)\n","    anim.save(path, writer=\"pillow\", fps = 30)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6700,"status":"ok","timestamp":1678911219045,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"vn1szW2eeoM_","outputId":"6a1d3995-aa43-4ea0-85ab-10d1f7e0caa5"},"outputs":[],"source":["# Load model parameter\n","path = \"/content/drive/MyDrive/INF581-RL-Shared/model/A2C_parameter1250.2855.pkl\"\n","agent = ActorCritic().to(DEVICE)\n","agent.load_state_dict(torch.load(path))\n","agent.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"executionInfo":{"elapsed":80272,"status":"ok","timestamp":1678914349622,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"uT-QIcNbqEWo","outputId":"bc9aa47a-f05e-4e39-9c40-e4ef6ecdb963"},"outputs":[],"source":["total_reward = 0\n","frames = []\n","step = 0\n","\n","state = env.reset()\n","state = process_state(state)\n","\n","for step in range(MAX_STEP):\n","\n","    # Sample action\n","    action = agent.choose_action(state)\n","    # Step forward\n","    new_state, reward, done, info = env.step(action)\n","\n","    # Transform the state\n","    new_state = process_state(new_state)\n","\n","    # Add frames\n","    show_frame(env, step, info)\n","    frames.append(env.render(mode = \"rgb_array\"))\n","        \n","    total_reward += reward\n","\n","    state = new_state\n","\n","    if done and info['lives']==0:\n","      print(total_reward)\n","      break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"elapsed":14929,"status":"ok","timestamp":1678914400967,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"tfFweVQgqE3s","outputId":"59eec08e-0279-4660-ea25-c6344def45e4"},"outputs":[],"source":["path = \"/content/drive/MyDrive/INF581-RL-Shared/gif/A2C_1250.3050.gif\"\n","save_gif(frames, path)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMXAtzJ4NwkCSaV97KzFgI7","collapsed_sections":["rqoBGzwhpBrA","8gJSIqWSpJys","0gvh3srMpUfA"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00f7ac4bedc945b8bca9f688b5b602b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2392f8b3f2cf41bbbce267baac80f293","max":499,"min":0,"orientation":"horizontal","style":"IPY_MODEL_510239ab1ceb4f39bf2767d2a23cc1bb","value":499}},"07bda516d15041ae92b7a630e425213c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8ba2315a48b4957a715f6abaeadb9d3","placeholder":"​","style":"IPY_MODEL_dfcfe1b71b304c368ee4d490945a5a9c","value":"100%"}},"2392f8b3f2cf41bbbce267baac80f293":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28d749822e684c93a3e25b9cd9369c7e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3074789bf027471383d0e46af91708b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"374f96f8122d48689b0bc08cc1b2b5dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a81f32990c941db8c3aa92ad3e4f06c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ba4943fdef4449e819c798d896ff86f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28d749822e684c93a3e25b9cd9369c7e","placeholder":"​","style":"IPY_MODEL_5331f7b3adde4421aa1f0f490d2f9c27","value":" 99/99 [08:23&lt;00:00,  4.80s/it]"}},"41e09cd5bb1e469aa46ac43f06bf9c85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4938f84e35544600879344f2f6c5e127","IPY_MODEL_00f7ac4bedc945b8bca9f688b5b602b1","IPY_MODEL_66bd9a1beacc419783ea62e041898512"],"layout":"IPY_MODEL_3074789bf027471383d0e46af91708b5"}},"4938f84e35544600879344f2f6c5e127":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_880dc584314b4de9bcd128474a271586","placeholder":"​","style":"IPY_MODEL_374f96f8122d48689b0bc08cc1b2b5dd","value":"100%"}},"510239ab1ceb4f39bf2767d2a23cc1bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5331f7b3adde4421aa1f0f490d2f9c27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a991549c659463b8beb4a6109eb56ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66bd9a1beacc419783ea62e041898512":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79bdf797765f4370b7544ef70b0c462e","placeholder":"​","style":"IPY_MODEL_5a991549c659463b8beb4a6109eb56ae","value":" 499/499 [09:11&lt;00:00,  1.53it/s]"}},"79bdf797765f4370b7544ef70b0c462e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"880dc584314b4de9bcd128474a271586":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b63b9648473740b6b8a0b33c309790d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8ba2315a48b4957a715f6abaeadb9d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfcfe1b71b304c368ee4d490945a5a9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e938c4ea1b23483cbfd40a131485c6bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a81f32990c941db8c3aa92ad3e4f06c","max":99,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eda2fae0dfc84f268f46e0fde7758e6f","value":99}},"ea735b5c7e024aa985123e8cf0b8a42f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07bda516d15041ae92b7a630e425213c","IPY_MODEL_e938c4ea1b23483cbfd40a131485c6bd","IPY_MODEL_3ba4943fdef4449e819c798d896ff86f"],"layout":"IPY_MODEL_b63b9648473740b6b8a0b33c309790d9"}},"eda2fae0dfc84f268f46e0fde7758e6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
