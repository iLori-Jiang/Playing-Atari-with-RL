{"cells":[{"attachments":{},"cell_type":"markdown","id":"07dc46e8","metadata":{},"source":["# Prepare"]},{"cell_type":"code","execution_count":null,"id":"zrHZcdFPEY70","metadata":{"id":"zrHZcdFPEY70"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"rwVVLj4RE4MM","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149560,"status":"ok","timestamp":1678957696021,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"rwVVLj4RE4MM","outputId":"f798e3de-a432-4470-9804-5041c0762d3d"},"outputs":[],"source":["!pip install gym[atari]\n","!pip install autorom[accept-rom-license]"]},{"cell_type":"code","execution_count":null,"id":"28a7379d-5c58-4894-8c21-8950478d0958","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7874,"status":"ok","timestamp":1678957715945,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"28a7379d-5c58-4894-8c21-8950478d0958","outputId":"5bf86f76-46d5-4e88-bb57-020092482211"},"outputs":[],"source":["import gym\n","\n","import numpy as np\n","from collections import deque\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","import time\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from IPython import display\n","\n","\n","print(tf.__version__)\n","print(tf.config.list_physical_devices('GPU'))"]},{"cell_type":"markdown","id":"1713d3e2","metadata":{"id":"1713d3e2"},"source":["## Wrappers"]},{"cell_type":"code","execution_count":8,"id":"f24a5407","metadata":{"executionInfo":{"elapsed":1010,"status":"ok","timestamp":1678957784956,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"f24a5407"},"outputs":[],"source":["class ConcatObs(gym.Wrapper):\n","    def __init__(self, env, k=4):\n","        gym.Wrapper.__init__(self, env)\n","        self.k = k\n","        self.frames = deque([], maxlen=k)\n","        shp = env.observation_space.shape\n","        self.observation_space = gym.spaces.Box(low=0, high=255, shape=((k,) + shp), dtype=env.observation_space.dtype)\n","\n","    def reset(self):\n","        ob = self.env.reset()\n","        for _ in range(self.k):\n","            self.frames.append(ob)\n","\n","        return self._get_ob()\n","\n","    def step(self, action):\n","        total_reward = 0.0\n","        done = None\n","        for i in range(self.k):\n","            obs, reward, done, info = self.env.step(action)\n","            total_reward += reward\n","            self.frames.append(obs)\n","            \n","            # only count one live each episode\n","            done = True if info['lives'] < 4 else False\n","            if done:  \n","                break\n","        return self._get_ob(), total_reward, done, info\n","\n","    def _get_ob(self):\n","        return np.array(self.frames)"]},{"cell_type":"code","execution_count":9,"id":"6a7cfe11","metadata":{"executionInfo":{"elapsed":415,"status":"ok","timestamp":1678957788167,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"6a7cfe11"},"outputs":[],"source":["# A bunch of wrappers to get us started, please use these\n","class ObservationWrapper(gym.ObservationWrapper):\n","    def __init__(self, env, GRAYSCALE=False, NORMALIZE=False):\n","        self.GRAYSCALE = GRAYSCALE\n","        self.NORMALIZE = NORMALIZE\n","        super().__init__(env)\n","    \n","    def observation(self, obs):\n","        # Normalise observation by 255\n","        if self.NORMALIZE:\n","            obs = obs / 255.0\n","            \n","        if self.GRAYSCALE:\n","            obs = tf.image.rgb_to_grayscale(obs)\n","                    \n","        image = obs[:,2:-9,8:,:]\n","        image = tf.image.resize(image,[84,84])\n","        image = tf.transpose(tf.reshape(image, image.shape[:-1]),perm = [1,2,0])\n","        return image\n","\n","class RewardWrapper(gym.RewardWrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","    \n","    def reward(self, reward):\n","        # Clip reward between 0 to 1\n","        #return np.clip(reward, 0, 1)\n","        return reward\n","    \n","class ActionWrapper(gym.ActionWrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","    \n","    def action(self, action):\n","        return action\n","\n","class FireResetEnv(gym.Wrapper):\n","    def __init__(self, env):\n","        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n","        super().__init__(env)\n","        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n","        assert len(env.unwrapped.get_action_meanings()) >= 3\n","\n","    def reset(self, **kwargs):\n","        self.env.reset(**kwargs)\n","        obs, _, done, _ = self.env.step(1)\n","        if done:\n","            self.env.reset(**kwargs)\n","        obs, _, done, _ = self.env.step(2)\n","        if done:\n","            self.env.reset(**kwargs)\n","        return obs\n","\n","    def step(self, ac):\n","        return self.env.step(ac)"]},{"cell_type":"markdown","id":"Be0-5-XqWwQi","metadata":{"id":"Be0-5-XqWwQi"},"source":["# Environment"]},{"cell_type":"code","execution_count":10,"id":"38eb8dfc-80eb-4161-8c6c-c472ebaa3d9d","metadata":{"executionInfo":{"elapsed":460,"status":"ok","timestamp":1678957792512,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"38eb8dfc-80eb-4161-8c6c-c472ebaa3d9d"},"outputs":[],"source":["env = gym.make(\"ALE/Riverraid-v5\")"]},{"cell_type":"code","execution_count":11,"id":"929c1702","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678957796494,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"929c1702"},"outputs":[],"source":["# Use wrappers for the environment\n","env = ObservationWrapper(RewardWrapper(ActionWrapper(ConcatObs(FireResetEnv(env),k=4))), GRAYSCALE=True, NORMALIZE=True)\n","obs = env.reset()"]},{"cell_type":"markdown","id":"90535de1","metadata":{"id":"90535de1"},"source":["# DQN"]},{"cell_type":"markdown","id":"KlQ2DhPvPie2","metadata":{"id":"KlQ2DhPvPie2"},"source":["## Build model"]},{"cell_type":"code","execution_count":12,"id":"c-pevj43Vp7O","metadata":{"executionInfo":{"elapsed":397,"status":"ok","timestamp":1678957802832,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"c-pevj43Vp7O"},"outputs":[],"source":["hidden_size = 512\n","n_action = env.action_space.n"]},{"cell_type":"markdown","id":"4enH-XvOIq4_","metadata":{"id":"4enH-XvOIq4_"},"source":["### Orignial DQN"]},{"cell_type":"code","execution_count":13,"id":"rwWU4hP4H_lI","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1678957806652,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"rwWU4hP4H_lI"},"outputs":[],"source":["def create_dqn(input_shape, hidden_size, n_action):\n","    inputs = layers.Input(shape=input_shape)\n","    cnn1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n","    cnn2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(cnn1)\n","    cnn3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(cnn2)\n","    flatten = layers.Flatten()(cnn3)\n","    \n","    adv_dense1 = keras.layers.Dense(hidden_size, activation='relu')(flatten) \n","    # val_dense1 = keras.layers.Dense(hidden_size, activation='relu')(flatten)\n","    \n","    adv_out = keras.layers.Dense(n_action)(adv_dense1)\n","    # val_out = keras.layers.Dense(1)(val_dense1)\n","    \n","    # out = tf.math.add(adv_out, val_out - tf.reduce_mean(adv_out))\n","    \n","    return keras.Model(inputs=inputs, outputs=adv_out)"]},{"cell_type":"markdown","id":"DbXG-oNGI_gO","metadata":{"id":"DbXG-oNGI_gO"},"source":["## Dueling DQN"]},{"cell_type":"code","execution_count":14,"id":"jvUwG4XLWJ5f","metadata":{"executionInfo":{"elapsed":459,"status":"ok","timestamp":1678957840212,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"jvUwG4XLWJ5f"},"outputs":[],"source":["def create_dueling_dqn(input_shape, hidden_size, n_action):\n","    inputs = layers.Input(shape=input_shape)\n","    cnn1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n","    cnn2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(cnn1)\n","    cnn3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(cnn2)\n","    flatten = layers.Flatten()(cnn3)\n","    \n","    adv_dense1 = keras.layers.Dense(hidden_size, activation='relu')(flatten) \n","    val_dense1 = keras.layers.Dense(hidden_size, activation='relu')(flatten)\n","    \n","    adv_out = keras.layers.Dense(n_action)(adv_dense1)\n","    val_out = keras.layers.Dense(1)(val_dense1)\n","    \n","    out = tf.math.add(adv_out, val_out - tf.reduce_mean(adv_out))\n","    \n","    return keras.Model(inputs=inputs, outputs=out)"]},{"cell_type":"code","execution_count":null,"id":"10ed81d9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":454,"status":"ok","timestamp":1678957842946,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"10ed81d9","outputId":"95a9d2d6-ea5b-4bb1-b80a-10d7a2d7a375"},"outputs":[],"source":["qmodel = create_dueling_dqn(input_shape=obs.shape, hidden_size=hidden_size, n_action=n_action)\n","print(qmodel.summary())"]},{"cell_type":"code","execution_count":null,"id":"C9xcb8yAJVnh","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1682,"status":"ok","timestamp":1678957848136,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"C9xcb8yAJVnh","outputId":"5f095a0a-848a-4f64-dce4-97c34e79502d"},"outputs":[],"source":["qmodel = create_dqn(input_shape=obs.shape, hidden_size=hidden_size, n_action=n_action)\n","print(qmodel.summary())"]},{"cell_type":"markdown","id":"chPTaDiMUFb9","metadata":{"id":"chPTaDiMUFb9"},"source":["# Hyper parameters"]},{"cell_type":"code","execution_count":17,"id":"b1053a5c","metadata":{"executionInfo":{"elapsed":393,"status":"ok","timestamp":1678957871526,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"b1053a5c"},"outputs":[],"source":["# Hyper parameters\n","max_step = 100000\n","gamma = 0.99\n","decay_rate = 0.995\n","eps = 0.99\n","eps_threshold = 0.1\n","tau = 5\n","check_point = 100\n","\n","MEM_SIZE = 10000\n","BATCH_SIZE = 64"]},{"cell_type":"markdown","id":"d9ZWNx07XGyW","metadata":{"id":"d9ZWNx07XGyW"},"source":["# Train"]},{"cell_type":"code","execution_count":23,"id":"21ca50e5-2251-41c5-b99e-355688a4cd7b","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1678957912776,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"21ca50e5-2251-41c5-b99e-355688a4cd7b"},"outputs":[],"source":["def take_action(env, q_values, state, eps=0.1, greedy=False):\n","    if not greedy and np.random.rand(1) < eps:\n","        action = np.random.randint(n_action)\n","    else:\n","        action = tf.math.argmax(q_values, axis=1).numpy()[0] # greedy\n","    return action\n","\n","\n","def train_one_episode(env, qmodel, target_model, trainer, eps, gamma, tau,\n","                      max_step, is_double=False,\n","                      replay_buffer=None, batch_size=BATCH_SIZE):\n","    start_time = time.time()\n","    total_reward = 0\n","    obs = env.reset()\n","    step = 0\n","\n","    for step in range(max_step):\n","        with tf.GradientTape() as tape:\n","            q_values = qmodel(np.array([obs]))\n","            action = take_action(env,q_values ,obs ,eps)\n","\n","            obs_new,reward ,done ,info=env.step(action)\n","            done=True if info['lives']<4 else False\n","\n","            # Add experience to replay buffer\n","            if replay_buffer is not None:\n","                replay_buffer.add(obs=obs,\n","                                  action=action,\n","                                  reward=reward,\n","                                  next_obs=obs_new,\n","                                  done=done)\n","\n","            # Sample from replay buffer and update model\n","            if replay_buffer is not None and len(replay_buffer) > batch_size:\n","                experiences = replay_buffer.sample(batch_size)\n","                obses = experiences['obs']\n","                actions = experiences['action']\n","                rewards = experiences['reward']\n","                next_obses = experiences['next_obs']\n","                dones = experiences['done']\n","\n","                q_values_next_target=tf.stop_gradient(target_model(next_obses))\n","                \n","                if is_double:\n","                    q_values_next_online=qmodel(next_obses)\n","                    actions_next=tf.argmax(q_values_next_online,axis=1)\n","                else:\n","                    actions_next=tf.argmax(q_values_next_target,axis=1)\n","\n","                action_q=tf.reduce_sum(qmodel(obses)*tf.one_hot(actions,n_action),axis=1)\n","                \n","                action_q_next=tf.reduce_sum(q_values_next_target*tf.one_hot(actions_next,n_action),axis=1)\n","                \n","                action_target=rewards+(1-tf.cast(dones ,tf.float32))*gamma*action_q_next\n","                \n","                error=action_target-action_q\n","                loss=tf.reduce_sum(tf.square(error))\n","                \n","            else: # If no replay buffer or not enough samples yet\n","                q_values_next_target=tf.stop_gradient(target_model(np.array([obs_new])))\n","\n","                if is_double:\n","                    q_values_next_online=qmodel(np.array([obs_new]))\n","                    action_next=tf.argmax(q_values_next_online,axis=1)\n","                else:\n","                    action_next=tf.argmax(q_values_next_target,axis=1)\n","                    \n","                action_q=tf.reduce_sum(q_values*tf.one_hot(action,n_action),axis=1)\n","                \n","                action_q_next=tf.reduce_sum(q_values_next_target*tf.one_hot(action_next,n_action),axis=1)\n","                \n","                action_target=reward+(1-tf.cast(done ,tf.float32))*gamma*action_q_next\n","                \n","                error=action_target-action_q\n","                loss=tf.reduce_sum(tf.square(error))\n","                    \n","        grad=tape.gradient(loss,qmodel.trainable_variables)\n","        trainer.apply_gradients(zip(grad,qmodel.trainable_variables))\n","        total_reward+=reward\n","        \n","        obs=obs_new\n","        if done:\n","            break\n","        \n","        if (step+1)%tau==0:\n","            target_model.set_weights(qmodel.get_weights())\n"," \n","    end_time=time.time()\n","    runtime=end_time-start_time\n","    \n","    return total_reward,runtime ,step"]},{"cell_type":"code","execution_count":24,"id":"WfhJO9vVGjZM","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1678957916765,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"WfhJO9vVGjZM"},"outputs":[],"source":["def show_state(env, step=0, info=\"\"):\n","    plt.figure(3)\n","    plt.clf()\n","    plt.imshow(env.render(mode='rgb_array'))\n","    plt.title(\"%s | Step: %d %s\" % (env.spec.id, step, info))\n","    plt.axis('off')\n","\n","    display.clear_output(wait=True)\n","    display.display(plt.gcf())"]},{"cell_type":"code","execution_count":25,"id":"KKOUoACSSAM6","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1678957920436,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"KKOUoACSSAM6"},"outputs":[],"source":["class ReplayBuffer:\n","    def __init__(self, size):\n","        self.size = size\n","        self.buffer = []\n","        self.next_idx = 0\n","\n","    def add(self, obs, action, reward, next_obs, done):\n","        data = (obs, action, reward, next_obs, done)\n","\n","        if self.next_idx >= len(self.buffer):\n","            self.buffer.append(data)\n","        else:\n","            self.buffer[self.next_idx] = data\n","        self.next_idx = (self.next_idx + 1) % self.size\n","\n","    def sample(self, batch_size):\n","        idxs = np.random.randint(0,len(self.buffer),size=batch_size)\n","        obses ,actions ,rewards ,next_obses ,dones= [],[],[],[],[]\n","        \n","        for idx in idxs:\n","            data=self.buffer[idx]\n","            obs ,action ,reward ,next_obs ,done=data\n","            obses.append(np.array(obs,copy=False))\n","            actions.append(np.array(action,copy=False))\n","            rewards.append(reward)\n","            next_obses.append(np.array(next_obs,copy=False))\n","            dones.append(done)\n","            \n","        return dict(obs=np.array(obses),\n","                    action=np.array(actions),\n","                    reward=np.array(rewards),\n","                    next_obs=np.array(next_obses),\n","                    done=np.array(dones))\n","\n","    def __len__(self):\n","        return len(self.buffer)"]},{"cell_type":"code","execution_count":26,"id":"MNEBympm74V-","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1678957923101,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"MNEBympm74V-"},"outputs":[],"source":["def train(check_point, env, qmodel, target_model, trainer, eps, decay_rate, eps_threshold, gamma, tau, max_step, n_iteration, save_name, is_double=False, replay_size=-1):\n","    reward_list = []\n","    runtime_list = []\n","    n_step_list = []\n","\n","    avg_reward_list = []\n","\n","    eps = eps\n","\n","    if(replay_size>0):\n","        replay_buffer = ReplayBuffer(replay_size)\n","    else:\n","        replay_buffer = None\n","\n","    p_bar = tqdm(range(n_iteration))\n","    for i in p_bar:\n","        eps = max(eps*decay_rate, eps_threshold)\n","        reward, runtime, n_step = train_one_episode(env, qmodel, target_model, trainer, eps, gamma, tau, max_step, is_double, replay_buffer)\n","        reward_list.append(reward)\n","        runtime_list.append(runtime)\n","        n_step_list.append(n_step)\n","\n","        if (i+1) % check_point == 0:\n","            avg_reward = np.mean(reward_list[-check_point:])\n","            avg_reward_list.append(avg_reward)\n","            p_bar.set_postfix_str(f\"Iteration:{i} current reward:{reward_list[-1]} current n_step:{n_step_list[-1]} last {check_point} average reward:{avg_reward}\", refresh=False)\n","    \n","    qmodel.save(save_name+'.h5') \n","    np.save(save_name+'_reward.npy', reward_list)\n","    np.save(save_name+'_avg_reward.npy', avg_reward_list)\n","    np.save(save_name+'_runtime.npy', runtime_list)\n","    np.save(save_name+'_step.npy', n_step_list)\n","    return avg_reward_list, reward_list, runtime_list, n_step_list"]},{"cell_type":"markdown","id":"4QpdY1UHN2pq","metadata":{"id":"4QpdY1UHN2pq"},"source":["# Train for DQN, without Dueling network, without Double"]},{"cell_type":"code","execution_count":null,"id":"z2OWNLeaxlSZ","metadata":{"id":"z2OWNLeaxlSZ"},"outputs":[],"source":["qmodel = create_dqn(input_shape=obs.shape, hidden_size=hidden_size, n_action=n_action)\n","target_model = create_dqn(input_shape=obs.shape, hidden_size=hidden_size, n_action=n_action)\n","trainer = keras.optimizers.Adam(learning_rate=0.001)"]},{"cell_type":"code","execution_count":null,"id":"efMF5V49_i1C","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2271231,"status":"ok","timestamp":1678956086833,"user":{"displayName":"永遠","userId":"14424450187549465523"},"user_tz":-60},"id":"efMF5V49_i1C","outputId":"14683150-8027-464c-d166-820c422f589c"},"outputs":[],"source":["n_iteration = 3000\n","save_name = \"DQN\"\n","avg_reward_list, reward_list, runtime_list, n_step_list = train(check_point, env, qmodel, target_model, trainer, eps, decay_rate, eps_threshold, gamma, tau, max_step, n_iteration, save_name, is_double=False)"]},{"cell_type":"code","execution_count":null,"id":"59c895c1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":625},"executionInfo":{"elapsed":1114,"status":"ok","timestamp":1678956088122,"user":{"displayName":"永遠","userId":"14424450187549465523"},"user_tz":-60},"id":"59c895c1","outputId":"d5939c19-b861-4fe4-f7fa-f8577496a50f"},"outputs":[],"source":["print(\"average_reward:{}\".format(sum(reward_list)/n_iteration))\n","print(\"average_time:{}\".format(sum(runtime_list)/n_iteration))\n","print(\"max_step:{}\".format(max(n_step_list)))\n","\n","plt.plot(reward_list)\n","plt.title('Reward for episodes')\n","plt.ylabel('Reward')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/rewardDDQN.jpg')\n","plt.show()\n","\n","plt.plot(n_step_list, color='green')\n","plt.title('Run steps for episodes')\n","plt.ylabel('Steps')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/runtimeDDQN.jpg')\n","plt.show()"]},{"cell_type":"markdown","id":"qM36tbq5JHSl","metadata":{"id":"qM36tbq5JHSl"},"source":["# Train for DQN, with Dueling network, without double"]},{"cell_type":"code","execution_count":null,"id":"DEURU8zlPvUT","metadata":{"id":"DEURU8zlPvUT"},"outputs":[],"source":["qmodel = create_dueling_dqn(input_shape=obs.shape, hidden_size=hidden_size, n_action=n_action)\n","target_model = create_dueling_dqn(input_shape=obs.shape, hidden_size=hidden_size, n_action=n_action)\n","trainer = keras.optimizers.Adam(learning_rate=0.001)"]},{"cell_type":"code","execution_count":null,"id":"KHanjG2LPyzY","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHanjG2LPyzY","outputId":"4855d458-6037-4af3-bc4a-a7b1e46167bf"},"outputs":[],"source":["n_iteration = 3000\n","save_name = \"DuelingDQN\"\n","avg_reward_list, reward_list, runtime_list, n_step_list = train(check_point, env, qmodel, target_model, trainer, eps, decay_rate, eps_threshold, gamma, tau, max_step, n_iteration, save_name, is_double=False)"]},{"cell_type":"code","execution_count":null,"id":"Bb72ZeFxkZM5","metadata":{"id":"Bb72ZeFxkZM5"},"outputs":[],"source":["print(\"average_reward:{}\".format(sum(reward_list)/n_iteration))\n","print(\"average_time:{}\".format(sum(runtime_list)/n_iteration))\n","print(\"max_step:{}\".format(max(n_step_list)))\n","\n","plt.plot(reward_list)\n","plt.title('Reward for episodes')\n","plt.ylabel('Reward')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/rewardDDQN.jpg')\n","plt.show()\n","\n","plt.plot(n_step_list, color='green')\n","plt.title('Run steps for episodes')\n","plt.ylabel('Steps')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/runtimeDDQN.jpg')\n","plt.show()"]},{"cell_type":"markdown","id":"xXZwqEaZQTbI","metadata":{"id":"xXZwqEaZQTbI"},"source":["# Train for DQN, with Dueling Network, with Double"]},{"cell_type":"code","execution_count":null,"id":"Y2ys_JnyQaaT","metadata":{"id":"Y2ys_JnyQaaT"},"outputs":[],"source":["qmodel = create_dueling_dqn(input_shape=obs.shape, hidden_size=hidden_size, n_action=n_action)\n","target_model = create_dueling_dqn(input_shape=obs.shape, hidden_size=hidden_size, n_action=n_action)\n","trainer = keras.optimizers.Adam(learning_rate=0.001)"]},{"cell_type":"code","execution_count":null,"id":"OW_QUEjtQa1b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113381,"status":"ok","timestamp":1678934096761,"user":{"displayName":"永遠","userId":"14424450187549465523"},"user_tz":-60},"id":"OW_QUEjtQa1b","outputId":"35b871b7-23ef-47d6-8abe-f199790a1e3e"},"outputs":[],"source":["n_iteration = 3000\n","save_name = \"DoubleDuelingDQN\"\n","avg_reward_list, reward_list, runtime_list, n_step_list = train(check_point, env, qmodel, target_model, trainer, eps, decay_rate, eps_threshold, gamma, tau, max_step, n_iteration, save_name, is_double=True)"]},{"cell_type":"code","execution_count":null,"id":"r1kmvA51katc","metadata":{"id":"r1kmvA51katc"},"outputs":[],"source":["print(\"average_reward:{}\".format(sum(reward_list)/n_iteration))\n","print(\"average_time:{}\".format(sum(runtime_list)/n_iteration))\n","print(\"max_step:{}\".format(max(n_step_list)))\n","\n","plt.plot(reward_list)\n","plt.title('Reward for episodes')\n","plt.ylabel('Reward')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/rewardDDQN.jpg')\n","plt.show()\n","\n","plt.plot(n_step_list, color='green')\n","plt.title('Run steps for episodes')\n","plt.ylabel('Steps')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/runtimeDDQN.jpg')\n","plt.show()"]},{"cell_type":"markdown","id":"3EGcAhVzUmK7","metadata":{"id":"3EGcAhVzUmK7"},"source":["# Train for DQN, with Dueling Network, with Double, with replay buffer"]},{"cell_type":"code","execution_count":27,"id":"ACj0y940Uljl","metadata":{"executionInfo":{"elapsed":718,"status":"ok","timestamp":1678957932712,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"ACj0y940Uljl"},"outputs":[],"source":["qmodel = create_dueling_dqn(input_shape=obs.shape, hidden_size=hidden_size, n_action=n_action)\n","target_model = create_dueling_dqn(input_shape=obs.shape, hidden_size=hidden_size, n_action=n_action)\n","trainer = keras.optimizers.Adam(learning_rate=0.001)"]},{"cell_type":"code","execution_count":null,"id":"xCn_W_PbUstd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5446050,"status":"ok","timestamp":1678967969519,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"xCn_W_PbUstd","outputId":"3cee1907-108a-4833-c81b-93479ef66e2f"},"outputs":[],"source":["n_iteration = 3000\n","save_name = \"ReplayDoubleDuelingDQN\"\n","avg_reward_list, reward_list, runtime_list, n_step_list = train(check_point, env, qmodel, target_model, trainer, eps, decay_rate, eps_threshold, gamma, tau, max_step, n_iteration, save_name, is_double=False,replay_size=MEM_SIZE)"]},{"cell_type":"code","execution_count":null,"id":"qn8E8i1sD4g6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":403,"status":"ok","timestamp":1678968016245,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"qn8E8i1sD4g6","outputId":"f7339f28-ba0a-46b5-e66a-375f358b235a"},"outputs":[],"source":["qmodel.save('DuelingReplay'+ str(3000) + '.h5')"]},{"cell_type":"code","execution_count":null,"id":"X62FDAzWkcMZ","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":624},"executionInfo":{"elapsed":798,"status":"ok","timestamp":1678968025656,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"X62FDAzWkcMZ","outputId":"6fd7a994-33f0-419e-a40b-919d8dc8d476"},"outputs":[],"source":["print(\"average_reward:{}\".format(sum(reward_list)/n_iteration))\n","print(\"average_time:{}\".format(sum(runtime_list)/n_iteration))\n","print(\"max_step:{}\".format(max(n_step_list)))\n","\n","plt.plot(reward_list)\n","plt.title('Reward for episodes')\n","plt.ylabel('Reward')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/rewardDDQN.jpg')\n","plt.show()\n","\n","plt.plot(n_step_list, color='green')\n","plt.title('Run steps for episodes')\n","plt.ylabel('Steps')\n","plt.xlabel('Episodes')\n","# plt.savefig('/content/drive/MyDrive/INF581/runtimeDDQN.jpg')\n","plt.show()"]},{"cell_type":"markdown","id":"dpVqq34oTikn","metadata":{"id":"dpVqq34oTikn"},"source":["# Save training log"]},{"cell_type":"code","execution_count":null,"id":"32pGxpvOmqkC","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29801,"status":"ok","timestamp":1678968061569,"user":{"displayName":"iLori Jiang","userId":"11715883954671146826"},"user_tz":-60},"id":"32pGxpvOmqkC","outputId":"9533dede-15e1-4b8b-eb2a-ee43bd4e8d4b"},"outputs":[],"source":["import os\n","import shutil\n","\n","destination_folder = '/content/drive/MyDrive/'\n","\n","for file in os.listdir():\n","    if file.endswith('.npy'):\n","        shutil.move(file, destination_folder)\n","for file in os.listdir():\n","    if file.endswith('.h5'):\n","        shutil.move(file, destination_folder)"]},{"cell_type":"markdown","id":"rvGfLHmSKKok","metadata":{"id":"rvGfLHmSKKok"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"id":"tcTbJD6HHF3a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":741,"status":"ok","timestamp":1677205810414,"user":{"displayName":"永遠","userId":"14424450187549465523"},"user_tz":-60},"id":"tcTbJD6HHF3a","outputId":"40bd3e89-9331-49c0-a8c4-1d72d2d678fd"},"outputs":[],"source":["from keras.models import load_model\n","model = load_model('DDQN_v3.h5')\n","\n","target_model = create_ddqn(input_shape=obs.shape, hidden_size=hidden_size, n_action=n_action)\n","target_model.set_weights(model.get_weights())"]},{"cell_type":"code","execution_count":null,"id":"dkjXJ_XPHZTR","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"executionInfo":{"elapsed":47342,"status":"ok","timestamp":1677206127885,"user":{"displayName":"永遠","userId":"14424450187549465523"},"user_tz":-60},"id":"dkjXJ_XPHZTR","outputId":"57dbb37d-8dfe-4985-fb9b-5f06b0fb81eb"},"outputs":[],"source":["total_reward = 0\n","obs = env.reset()\n","for step in range(max_step):\n","    q_values = model.predict(np.array([obs]))\n","    action = take_action(env, q_values, obs, greedy=True)\n","\n","    obs_new, reward, done, info = env.step(action)\n","\n","    show_state(env, step, info)\n","\n","    q_values_new = tf.stop_gradient(target_model(np.array([obs_new])))\n","\n","    action_q = tf.reduce_sum(q_values * tf.one_hot(action, n_action), axis=1)\n","    action_next = tf.argmax(q_values_new, axis=1)\n","    action_q_next = tf.reduce_sum(q_values_new * tf.one_hot(action_next, n_action), axis=1)\n","    action_target = reward + (1 - tf.cast(done, tf.float32)) * gamma * action_q_next\n","\n","    error = action_target - action_q\n","\n","    loss = tf.reduce_sum(tf.square(error))\n","        \n","    total_reward += reward\n","\n","    obs = obs_new\n","    if done and info['lives']==0:\n","      print(total_reward)\n","      break"]},{"cell_type":"code","execution_count":null,"id":"WQ813sSLyTjk","metadata":{"id":"WQ813sSLyTjk"},"outputs":[],"source":["avg_rewards_plot = [154.2,\n","  191.3,\n","  201.1,\n","  187.3,\n","  97.9,\n","  299.3,\n","  384.9,\n","  232.3,\n","  285.5,\n","  114.4,\n","  106.0,\n","  247.3,\n","  197.3,\n","  101.1,\n","  286.5,\n","  386.3,\n","  452.6,\n","  386.9,\n","  463.0,\n","  401.4,\n","  468.2,\n","  525.4,\n","  552.9,\n","  453.0,\n","  481.5,\n","  529.8,\n","  518.8,\n","  532.2,\n","  533.8,\n","  523.4,\n","  361.0,\n","  557.9,\n","  505.7,\n","  499.4,\n","  487.8,\n","  468.3,\n","  494.9,\n","  551.5,\n","  447.1,\n","  540.7,\n","  493.8,\n","  433.4,\n","  541.6,\n","  566.7,\n","  528.6,\n","  541.1,\n","  460.1,\n","  379.5,\n","  387.0,\n","  434.3]"]},{"cell_type":"code","execution_count":null,"id":"57wmRiE2yqdI","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677265416124,"user":{"displayName":"永遠","userId":"14424450187549465523"},"user_tz":-60},"id":"57wmRiE2yqdI","outputId":"80fb5886-f84c-49fc-f3cb-e51043e45c85"},"outputs":[],"source":["fig = plt.figure()\n","plt.plot(range(0,5000,100),avg_rewards_plot)\n","plt.xlabel(\"episode\")\n","plt.ylabel(\"avg rewards in 100 episode\")\n","plt.title(\"Average reward for episodes\")\n","fig.savefig(\"avg_reward_episode\")"]}],"metadata":{"@webio":{"lastCommId":null,"lastKernelId":null},"accelerator":"GPU","colab":{"collapsed_sections":["4QpdY1UHN2pq","qM36tbq5JHSl","xXZwqEaZQTbI","rvGfLHmSKKok"],"provenance":[{"file_id":"14rNXAa7gOlifC0Kwzha6Qk65tgwVOYlM","timestamp":1678957432829}]},"environment":{"kernel":"python3","name":"tf2-gpu.2-8.m91","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"},"gpuClass":"standard","interpreter":{"hash":"b8bd69ed4ffee0ae412486d98ceaaadb3fa2922e6e180ca66d279a33125fa193"},"kernelspec":{"display_name":"atari","language":"python","name":"atari"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":5}
